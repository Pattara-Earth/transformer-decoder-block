{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5a88b3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a7986966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters config\n",
    "context_length = 256\n",
    "batch_size = 64\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "n_embd = 384\n",
    "num_heads = 6\n",
    "lr = 3e-4\n",
    "max_new_token = 500\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1f21a1",
   "metadata": {},
   "source": [
    "# Tinyshakespeare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "16514656",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "text = res.content.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9fd184cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(f'length of dataset in characters: {len(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2035dd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3aebee0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz \n",
      "Number of token: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"{''.join(chars)} \\nNumber of token: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5641dff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text tokenization in character level\n",
    "itos = {i:c for i, c in enumerate(chars)}\n",
    "stoi = {c:i for i, c in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: string to list of int\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: list of int to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "aed7d08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 54, 58, 47, 51, 59, 51, 1, 54, 56, 47, 42, 43, 43, 43, 2, 2, 2]\n",
      "Optimum prideee!!!\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"Optimum prideee!!!\"))\n",
    "print(decode(encode(\"Optimum prideee!!!\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "327e7f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ff921d",
   "metadata": {},
   "source": [
    "# Train and Val split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3b398599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test split\n",
    "n = int(0.9*len(data))\n",
    "train = data[:n]\n",
    "test = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8d74fb53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
       "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
       "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
       "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
       "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
       "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
       "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
       "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
       "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
       "        50, 50, 10,  0, 35])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:context_length+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "06389bad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train if split == 'train' else test\n",
    "    ix = torch.randint(len(data) - context_length, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+context_length] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+context_length+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "# for b in range(batch_size):\n",
    "#     for t in range(context_length):\n",
    "#         context = xb[b, :t+1]\n",
    "#         target = yb[b, t]\n",
    "#         print(f\"Input: {context} | target: {target}\")\n",
    "#     print(\"---\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f58c36",
   "metadata": {},
   "source": [
    "# Single head self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "bd2e6ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single head self-attention\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, n_embd, head_size, context_length):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length)))\n",
    "        \n",
    "        self.head_size = head_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape # (B, T, C) C=n_embd\n",
    "        k = self.key(x) # (B, T, C) @ (C, H) = (B, T, H)\n",
    "        q = self.query(x) # (B, T, C) @ (C, H) = (B, T, H)\n",
    "        # attention scores (affinities)\n",
    "        att = q @ k.transpose(-2, -1) / self.head_size**0.5\n",
    "        att = att.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        # weighted sum of the values\n",
    "        v = self.value(x) # (B, T, C) @ (C, H) = (B, T, H)\n",
    "        out = att @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29e98e2",
   "metadata": {},
   "source": [
    "# Multi-head self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ac1722f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, n_embd, head_size, context_length):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(n_embd, head_size, context_length) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(num_heads * head_size, num_heads * head_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff93b9d",
   "metadata": {},
   "source": [
    "# Feed-Forward Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0b288c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, fan_in, fan_out):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "                nn.Linear(fan_in, 4 * fan_out),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4 * fan_out, fan_out),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d19a309",
   "metadata": {},
   "source": [
    "# Decoder Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "200fe784",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, num_heads, n_embd, head_size, context_length):\n",
    "        super().__init__()\n",
    "        self.sd_heads = MultiHeadAttention(num_heads, n_embd, head_size, context_length)\n",
    "        self.ffwd = FeedForward(n_embd, n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.sd_heads(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d40b4c",
   "metadata": {},
   "source": [
    "# Bigram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2d62b374",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                     vocab_size, \n",
    "                     n_embd, \n",
    "                     num_heads, # heads params\n",
    "                     head_size, # heads params\n",
    "                     context_length \n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.token_embbeding_table = nn.Embedding(vocab_size, n_embd) # (B, T) -> (B, T, C)\n",
    "        self.position_embbeding_table = nn.Embedding(context_length, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "                Block(num_heads, n_embd, head_size, context_length),\n",
    "                Block(num_heads, n_embd, head_size, context_length),\n",
    "                Block(num_heads, n_embd, head_size, context_length),\n",
    "                Block(num_heads, n_embd, head_size, context_length),\n",
    "                Block(num_heads, n_embd, head_size, context_length),\n",
    "                Block(num_heads, n_embd, head_size, context_length),\n",
    "                nn.LayerNorm(n_embd)\n",
    "        )\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        self.context_length = context_length\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        token_emb = self.token_embbeding_table(idx) # (B, T, C)  C=n_embd\n",
    "        pos_emb = self.position_embbeding_table(torch.arange(T, device=device)) # (T, C) C=n_embd\n",
    "        x = token_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x) # (B, T, C) C=vocab_size\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # change logits shape for cross entropy loss (B, C)\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "        \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.context_length:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # (B, C)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            idx = torch.concat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e95d2415",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel(\n",
    "                                        vocab_size=vocab_size,\n",
    "                                        n_embd=n_embd,\n",
    "                                        num_heads=num_heads,\n",
    "                                        head_size=n_embd // num_heads,\n",
    "                                        context_length=context_length\n",
    "            ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4bd33a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10788929"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c4d66cbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bJ5xp\\AppData\\Local\\Temp\\ipykernel_9996\\3743136711.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm_notebook(range(epochs)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f78ace0ad73486fa3d6e42741d64577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.287578105926514\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm_notebook(range(epochs)):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1bef7a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "and wo ticherge naive desens, alld papay my ar dive, ffore wen try him fordsiat or, and wase sowr with ait so hus me are treingear may,\n",
      "An sconhe and fich sonith\n",
      "I os nese'd epper pleesse,\n",
      "Whan witte cee soneow we't me arwagh.\n",
      "\n",
      "LOUS: wia we you hish We thes muse oun with deilse ry lity the for stry me do ad ley yeepel of a'd Foke\n",
      "Corke deares,\n",
      "I'll olst that you ace he caway;\n",
      "Toslaize! ar! ELorjow he\n",
      "\n",
      "garcuntlearwith the mest habilan wis. hey my I bow got at fir\n",
      "For mu-akre baan, I we?\n",
      "\n",
      "Ase wo?\n",
      "\n",
      "MOV will is or leivowparaw with will'.\n",
      "\n",
      "KI bathis noticee hiss kis: ad, his is woor wo when any my ast his coud ther ebs.\n",
      "\n",
      "Butixswle, and mowrrinfewere sear andlarby fear and che shed\n",
      "What brat. ce my home ader and cany bly yourowspener, of pounk, g; Porce swore's\n",
      "Wootill noteades.\n",
      "Thirase,\n",
      "As of o, met buse lioour capunciruges:er,\n",
      "Rouhke are! as themavee.\n",
      "\n",
      "Then of qruce.\n",
      "\n",
      "Kh an delot ary wis arsage my sbrounes, igheran en my knoake fore lied the she hit marnys enceand ecan uh, thou.\n",
      "\n",
      "NRUEEN EY\n"
     ]
    }
   ],
   "source": [
    "out = decode(model.generate(torch.zeros((1,1), dtype=torch.long), 1000)[0].tolist())\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2284f954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before feed-forward 2.160\n",
    "# after feed-forward 2.549\n",
    "# block 2.354\n",
    "# add residual connect 1.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a51117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
